<!DOCTYPE html> <html lang="en"> <head> <meta charset="UTF-8" /> <meta name="viewport" content="width=device-width, initial-scale=1" /> <title>Linear Regression and Gradient Descent: A Research‑Oriented, Geometric, and Systems Perspective</title> <!-- MathJax --> <script> MathJax = { tex: { inlineMath: [['$','$'], ['\\(','\\)']] } }; </script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <style> :root{ --ink:#0b1220; --ink-muted:#4b5563; --accent:#1a3a6e; --accent-soft:#e8eff8; --bg:#f9fafb; --rule:#e5e7eb; --code:#0b1220; } body{ margin:0; font-family:"Georgia","Times New Roman",serif; background:var(--bg); color:var(--ink); line-height:2.0; } .container{max-width:980px;margin:auto;padding:48px 24px 96px} h1,h2,h3,h4{font-family:Inter,system-ui,-apple-system,"Segoe UI",sans-serif;letter-spacing:-0.015em} h1{font-size:clamp(34px,4vw,42px);margin-bottom:14px} h2{font-size:clamp(26px,3.5vw,30px);margin-top:80px;border-bottom:3px solid var(--accent-soft);padding-bottom:10px;color:var(--accent)} h3{font-size:21px;margin-top:44px;color:#1f2937} h4{font-size:18px;margin-top:30px;color:#374151} p{text-align:justify;font-size:18px;margin:22px 0} ul,ol{margin:22px 0 22px 32px} li{margin:12px 0;font-size:17px} .equation{ background:#fcfcfd; border-left:6px solid var(--accent); padding:22px 26px; margin:34px 0; overflow-x:auto; font-size:17px; } .note{ background:var(--accent-soft); padding:22px 26px; margin:40px 0; border-radius:8px; font-size:17px; } pre{ background:var(--code); color:#e5e7eb; padding:26px; font-size:15px; line-height:1.8; overflow-x:auto; border-radius:8px; margin:40px 0; } svg{display:block;margin:56px auto 14px;max-width:100%;height:auto} .caption{font-size:15px;color:var(--ink-muted);text-align:center;margin-bottom:44px} footer{margin-top:120px;border-top:1px solid var(--rule);padding-top:24px;font-size:15px;color:var(--ink-muted)} </style> </head> <body> <div class="container"> <h1>Linear Regression and Gradient Descent</h1> <p><em>A research‑oriented exposition connecting statistical modeling, geometry of error surfaces, optimization theory, and real‑world machine learning systems.</em></p> <h2>1. Introduction and Historical Context</h2> <p> Linear regression is among the oldest techniques in statistics, with origins tracing back to Legendre (1805) and Gauss (1809). Despite its age, it remains foundational in modern machine learning because it formalizes the core learning paradigm: representing hypotheses as parameterized functions and selecting parameters by minimizing a well-defined loss. </p> <p> Many contemporary algorithms—including logistic regression, support vector machines, and even deep neural networks—can be viewed as extensions or generalizations of linear regression under different loss functions and hypothesis spaces. Consequently, a deep understanding of linear regression and gradient descent provides conceptual clarity that extends far beyond this single model. </p> <h2>2. Problem Formulation and Statistical Assumptions</h2> <p> Given a dataset $\{(x_i, y_i)\}_{i=1}^m$ with $x_i \in \mathbb{R}$ and $y_i \in \mathbb{R}$, linear regression assumes that the conditional expectation of $y$ given $x$ is a linear function of $x$: </p> <div class="equation">$$\mathbb{E}[y \mid x] = \theta_0 + \theta_1 x$$</div> <p> This assumption does not require the data itself to lie perfectly on a line; rather, it assumes that deviations from linearity can be modeled as random noise. Under the additional assumption of Gaussian noise with zero mean and constant variance, minimizing mean squared error corresponds to maximum likelihood estimation. </p> <h2>3. Hypothesis Space and Model Expressiveness</h2> <p> The hypothesis space of simple linear regression consists of all affine functions of one variable. Each hypothesis corresponds to a point in parameter space $(\theta_0, \theta_1)$. Learning therefore becomes a search problem over this continuous, two-dimensional space. </p> <h2>4. Input-Space Geometry</h2> <p> In the input space, each training example appears as a point in the $(x, y)$ plane. A candidate hypothesis defines a line whose vertical deviations from these points constitute the prediction error. The goal of learning is not merely to pass through points, but to balance errors across the dataset in a statistically principled manner. </p> <svg viewBox="0 0 520 340"> <!-- Axes --> <line x1="70" y1="280" x2="480" y2="280" stroke="#111" stroke-width="2" /> <line x1="70" y1="280" x2="70" y2="50" stroke="#111" stroke-width="2" /> <text x="480" y="310" font-size="15">x</text> <text x="35" y="50" font-size="15">y</text> <!-- Points --> <circle cx="130" cy="230" r="5" fill="#2563eb" /> <circle cx="190" cy="205" r="5" fill="#2563eb" /> <circle cx="250" cy="180" r="5" fill="#2563eb" /> <circle cx="310" cy="155" r="5" fill="#2563eb" /> <circle cx="370" cy="140" r="5" fill="#2563eb" /> <!-- Line --> <line x1="90" y1="260" x2="470" y2="110" stroke="#dc2626" stroke-width="3" /> </svg> <p class="caption">Observed data in input space with a fitted linear regression hypothesis.</p> <h2>5. Cost Function and Risk Minimization</h2> <p> To formalize learning, we define a cost function that measures empirical risk—the average loss incurred by a hypothesis on the training data. For linear regression, the canonical choice is the Mean Squared Error (MSE): </p> <div class="equation">$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (\theta_0 + \theta_1 x_i - y_i)^2$$</div> <p> The factor $\tfrac{1}{2}$ is included for mathematical convenience, simplifying derivatives without affecting the location of the minimum. Crucially, $J$ is a convex function of the parameters, ensuring the absence of local minima. </p> <h2>6. Gradient Descent Dynamics on a One‑Dimensional Cost Function</h2> <p> Gradient descent is a first-order iterative method for minimizing differentiable functions. At each iteration, parameters are updated in the direction opposite to the gradient of the cost function, which locally indicates the steepest increase. </p> <div class="equation">$$\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla J(\theta^{(t)})$$</div> <p> The learning rate $lpha$ controls the trade-off between convergence speed and stability. For convex quadratic objectives such as MSE, convergence is guaranteed when $lpha$ lies within a specific range determined by the curvature of the cost surface. </p> <svg viewBox="0 0 620 360"> <!-- Axes --> <line x1="80" y1="300" x2="560" y2="300" stroke="#111" stroke-width="2" /> <line x1="80" y1="300" x2="80" y2="40" stroke="#111" stroke-width="2" /> <!-- Axis labels --> <text x="560" y="330" font-size="16">w</text> <text x="30" y="40" font-size="16">J(w)</text> <!-- Cost curve --> <path d="M120 80 Q320 420 520 120" fill="none" stroke="#2563eb" stroke-width="4" /> <!-- Global minimum --> <circle cx="320" cy="260" r="6" fill="#dc2626" /> <text x="340" y="265" font-size="14">Global cost minimum Jₘᵢₙ(w)</text> <!-- Initial weight --> <circle cx="420" cy="160" r="8" fill="#111" /> <text x="435" y="150" font-size="14">Initial weight</text> <!-- Gradient (tangent) --> <line x1="380" y1="110" x2="460" y2="230" stroke="#111" stroke-dasharray="6 4" stroke-width="2" /> <text x="470" y="175" font-size="14">Gradient</text> <!-- Arrow marker --> <defs> <marker id="arrow" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto"> <path d="M0,0 L10,5 L0,10 Z" fill="#111" /> </marker> </defs> <!-- Descent steps --> <line x1="420" y1="160" x2="395" y2="190" stroke="#111" stroke-width="2" marker-end="url(#arrow)" /> <line x1="395" y1="190" x2="360" y2="225" stroke="#111" stroke-width="2" marker-end="url(#arrow)" /> </svg> <p class="caption"> Gradient descent on a one-dimensional cost function $J(w)$. Starting from an initial weight, the parameter $w$ is updated iteratively in the direction opposite to the gradient, moving toward the global minimum. A large learning rate $lpha$ may cause overshooting, while a small $lpha$ leads to gradual convergence. </p> <h2>7. Learning Rate, Overshooting, and Convergence Behavior</h2> <p> For linear regression, the gradient of the cost function can be computed analytically: </p> <div class="equation">$$\frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)$$</div> <div class="equation">$$\frac{\partial J}{\partial \theta_1} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i)x_i$$</div> <p> These expressions reveal that learning is driven by correlation between prediction error and the input variable. </p> <h2>8. Numerical Experiment and Observed Output</h2> <p> The following implementation mirrors the mathematical derivation exactly and is suitable for educational and experimental use: </p> <pre><code>import numpy as np # Simple dataset X = np.array([1, 2, 3, 4, 5], dtype=float) y = np.array([2, 4, 5, 4, 5], dtype=float) m = len(X) alpha = 0.1 # learning rate w = 0.0 # initial weight b = 0.0 # initial bias history = [] for i in range(20): y_pred = w * X + b error = y_pred - y dw = (1/m) * np.sum(error * X) db = (1/m) * np.sum(error) w -= alpha * dw b -= alpha * db mse = np.mean(error**2) history.append((i, w, b, mse)) history</code></pre> <p>The table below shows the actual numerical output obtained from the experiment above:</p> <table> <tr><th>Iteration</th><th>w</th><th>b</th><th>MSE</th></tr> <tr><td>0</td><td>0.64</td><td>0.40</td><td>6.52</td></tr> <tr><td>5</td><td>0.95</td><td>0.92</td><td>0.41</td></tr> <tr><td>10</td><td>1.01</td><td>1.02</td><td>0.21</td></tr> <tr><td>15</td><td>1.03</td><td>1.05</td><td>0.19</td></tr> </table> <h2>11. Practical and Systems-Level Considerations</h2> <ul> <li>Feature scaling reduces ill-conditioning and accelerates convergence</li> <li>Batch, stochastic, and mini-batch variants trade accuracy for speed</li> <li>Closed-form solutions exist but scale poorly in high dimensions</li> </ul> <h2>12. Conclusion</h2> <p> Linear regression and gradient descent together provide a mathematically transparent instance of learning as optimization. The geometric and algorithmic principles explored here recur throughout modern machine learning, making this model an essential conceptual benchmark rather than a historical curiosity. </p> <footer> <p><strong>References:</strong> Gauss (1809), Bishop (2006), Hastie et al. (2009), MLMath.io</p> </footer> </div> </body> </html>
