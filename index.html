<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Linear Regression and Gradient Descent</title>

  <!-- MathJax -->
  <script>
    MathJax = { tex: { inlineMath: [['$','$'], ['\\(','\\)']] } };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root{
      --ink:#0b1220;
      --ink-muted:#4b5563;
      --accent:#1a3a6e;
      --accent-soft:#e8eff8;
      --bg:#f9fafb;
      --rule:#e5e7eb;
      --code:#0b1220;
    }

    body{
      margin:0;
      font-family:"Georgia","Times New Roman",serif;
      background:var(--bg);
      color:var(--ink);
      line-height:2.0;
    }

    .container{
      max-width:980px;
      margin:auto;
      padding:48px 24px 96px;
    }

    h1,h2,h3,h4{
      font-family:Inter,system-ui,-apple-system,"Segoe UI",sans-serif;
      letter-spacing:-0.015em;
    }

    h1{
      font-size:clamp(34px,4vw,42px);
      margin-bottom:14px;
    }

    h2{
      font-size:clamp(26px,3.5vw,30px);
      margin-top:80px;
      border-bottom:3px solid var(--accent-soft);
      padding-bottom:10px;
      color:var(--accent);
    }

    p{
      text-align:justify;
      font-size:18px;
      margin:22px 0;
    }

    .equation{
      background:#fcfcfd;
      border-left:6px solid var(--accent);
      padding:22px 26px;
      margin:34px 0;
      overflow-x:auto;
      font-size:17px;
    }

    pre{
      background:var(--code);
      color:#e5e7eb;
      padding:26px;
      font-size:15px;
      line-height:1.8;
      overflow-x:auto;
      border-radius:8px;
      margin:40px 0;
    }

    table{
      border-collapse:collapse;
      width:100%;
      margin:40px 0;
    }

    th,td{
      border:1px solid var(--rule);
      padding:12px;
      text-align:center;
    }

    th{
      background:var(--accent-soft);
    }

    /* Figure styling */
    .figure{
      margin:56px auto 44px;
      text-align:center;
    }

    .figure img{
      max-width:100%;
      height:auto;
      border-radius:6px;
      box-shadow:0 12px 30px rgba(0,0,0,0.12);
    }

    .figure .caption{
      font-size:15px;
      color:var(--ink-muted);
      margin-top:14px;
      line-height:1.7;
    }

    footer{
      margin-top:120px;
      border-top:1px solid var(--rule);
      padding-top:24px;
      font-size:15px;
      color:var(--ink-muted);
    }
  </style>
</head>

<body>
<div class="container">

<h1>Linear Regression and Gradient Descent</h1>
<p><em>A research-oriented exposition connecting geometry, optimization, and learning dynamics.</em></p>

<h2>6. Gradient Descent Dynamics on a One-Dimensional Cost Function</h2>

<p>
Gradient descent is a first-order optimization method that minimizes a differentiable
objective by iteratively moving parameters in the direction opposite to the gradient,
which locally represents the steepest ascent of the cost function.
</p>

<div class="equation">
$$
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla J(\theta^{(t)})
$$
</div>

<p>
The learning rate $\alpha$ determines the step size of each update. If $\alpha$ is too
small, convergence becomes slow; if it is too large, the algorithm may overshoot the
minimum or diverge.
</p>

<div class="figure">
  <img
    src="gradient-descent-1d-cost.png"
    alt="Gradient descent on a one-dimensional cost function"
  />
  <div class="caption">
    Gradient descent on a one-dimensional cost function $J(w)$. Starting from an initial
    weight, the parameter $w$ is iteratively updated in the direction opposite to the
    gradient (shown by the dashed tangent). The update steps move downhill along the cost
    curve toward the global minimum $J_{\min}(w)$. An excessively large learning rate
    $\alpha$ can cause overshooting, while a smaller $\alpha$ ensures stable but slower
    convergence.
  </div>
</div>

<h2>Conclusion</h2>
<p>
This geometric interpretation of gradient descent provides intuition that generalizes
directly to higher-dimensional parameter spaces, forming the conceptual foundation for
optimization in modern machine learning systems.
</p>

<footer>
<p><strong>References:</strong> Gauss (1809), Bishop (2006), Hastie et al. (2009), MLMath.io</p>
</footer>

</div>
</body>
</html>
